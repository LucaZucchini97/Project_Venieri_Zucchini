{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIKR module 3 project\n",
    "\n",
    "#Lorenzo Venieri\n",
    "#Luca Zucchini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Diabetes is a serious disease affecting millions of people across the entire world. Thus, correct and timely prediction of this disease is very important due to the complications it can have in the case of other life-threatening diseases.\n",
    "\n",
    "Objective of this project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "-Aggiungere funzione per trovare correlazione tra features\n",
    "-Considerare il fatto che il diabete causa durezza della pella, ma quest'ultima non è direttamente causa di diabete (=> != <=>)+\n",
    "-Piccola spiegazione del ragionamento dietro le correlazioni NON trovate\n",
    "-Fattore di rischio\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pgmpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#prva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "768 rows, 9 columns\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney\n",
    "Diseases. The objective of the dataset is to diagnostically predict whether a patient has diabetes,\n",
    "based on certain diagnostic measurements included in the dataset. Several constraints were placed\n",
    "on the selection of these instances from a larger database. In particular, all patients here are females\n",
    "at least 21 years old of Pima Indian heritage.\n",
    "From the data set in the (.csv) File We can find several variables, some of them are independent\n",
    "(several medical predictor variables) and only one target dependent variable (Outcome).\n",
    "\n",
    "We have 9 different attributes:\n",
    "1. Pregnancies: number of pregnancies\n",
    "2. Glucose: plasma glucose concentrarion \n",
    "3. BloodPressure: diastolic blood pressure mm/Hg\n",
    "4. SkinThickness: triceps skin fold thickness (mm)\n",
    "5. Insulin: insulin in U/mL\n",
    "6. BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes Pedigree Function: scores likelihood of diabetes based on family history\n",
    "8. Age: age of the person (years)\n",
    "9. Outcome: patient has diabetes (0 = No, 1 = Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_raw = pd.read_csv(\"data/diabetes.csv\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diabetes distribution in the dataset\n",
    "\n",
    "df_raw['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset there are 268 patients with diabetes and 500 without. The classes are imbalanced: only about 35% of the entries have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of the other 8 (continuous) features\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(18,16))\n",
    "axs = axs.flatten()\n",
    "sns.distplot(df_raw['Pregnancies'],rug=True,ax=axs[0])\n",
    "sns.distplot(df_raw['Glucose'],rug=True,ax=axs[1])\n",
    "sns.distplot(df_raw['BloodPressure'],rug=True,ax=axs[2])\n",
    "sns.distplot(df_raw['SkinThickness'],rug=True,ax=axs[3])\n",
    "sns.distplot(df_raw['Insulin'],rug=True,ax=axs[4])\n",
    "sns.distplot(df_raw['BMI'],rug=True,ax=axs[5])\n",
    "sns.distplot(df_raw['DiabetesPedigreeFunction'],rug=True,ax=axs[6])\n",
    "sns.distplot(df_raw['Age'],rug=True,ax=axs[7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che il dataset è sbilanciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numero di outcome seza diabete\", (df_raw[\"Outcome\"] == 0).sum())\n",
    "print(\"Numero di outcome con diabete\", (df_raw[\"Outcome\"] == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che ci sono tati valori nulli nel dataset. Sarebbero da togliere dato che sono dei NaN. Allo stesso tempo possiamo usare questa cosa per iniziare un downsample dei dati con Outcome '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_raw.columns:\n",
    "    counter = 0\n",
    "    for i in range(len(df_raw[item])):\n",
    "        if df_raw[item][i] == 0 and df_raw[\"Outcome\"][i] == 0:\n",
    "            counter += 1\n",
    "    print(\"nella colonna\", item,\"ci sono \",counter,\"righe da eliminare che possono semplicemente essere downsamplate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_raw.columns:\n",
    "    print(\"valori di \",item)\n",
    "    temp = df_raw[item].unique()\n",
    "    temp.sort()\n",
    "    #print(df_raw[item].unique())\n",
    "    print(temp)\n",
    "    print(\"----------------------\")\n",
    "    #\n",
    "    #chiedere a Sten dei bin sensati per questi valori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Togliamo le righe che hanno un valore nullo aka NaN e un outcome '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI','Age']\n",
    "for item in columns_to_clean:\n",
    "    drop_list = []\n",
    "    for i in range(len(df_clean[item])):\n",
    "        if df_clean[item][i] == 0 and df_clean[\"Outcome\"][i] == 0:\n",
    "            drop_list.append(i)\n",
    "    print(\"SIUUUM\")\n",
    "    df_clean.drop(labels= drop_list, axis= 0, inplace= True)\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo tolto un po di roba, adiamo sia a vedere se e quanto è sbilanciato il dataset che a togliere le restati righe che hanno un NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numero di outcome seza diabete\", (df_clean[\"Outcome\"] == 0).sum())\n",
    "print(\"Numero di outcome con diabete\", (df_clean[\"Outcome\"] == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllo sull'insulina e sul tipo di diabete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = 0\n",
    "count_2 = 1\n",
    "for i in range(df_clean.shape[0]):\n",
    "    if df_clean[\"Insulin\"][i] < 1 and df_clean[\"Outcome\"][i] == 1 :\n",
    "        count_1 += 1\n",
    "    if df_clean[\"Insulin\"][i] > 200 and df_clean[\"Outcome\"][i] == 1:\n",
    "        count_2 += 1\n",
    "print(\"presumibilmente ci sono \",count_1,\"pazienti con diabete di tipo 1\")\n",
    "print(\"presumibilmente ci sono \",count_2,\"pazienti con diabete di tipo 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Togliamo ora i restanti NaN senza però adare a toccare quelli dell'insulina ( presumiamo siano registrati nulli ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_clean.columns:\n",
    "    print(\"    - nella colonna \", item,\" sono presenti questi zeri\")\n",
    "    print((df_clean[item] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI','Age']\n",
    "\n",
    "for item in columns_to_clean:\n",
    "    drop_list = []\n",
    "    for i in range(len(df_clean[item])):\n",
    "        if df_clean[item][i] == 0 :\n",
    "            drop_list.append(i)\n",
    "    print(\"SIUUUM\")\n",
    "    df_clean.drop(labels= drop_list, axis= 0, inplace= True)\n",
    "    df_clean.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bene, andiamo a vedere cosa rimane del dataset originario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numero di outcome seza diabete\", (df_clean[\"Outcome\"] == 0).sum())\n",
    "print(\"Numero di outcome con diabete\", (df_clean[\"Outcome\"] == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come notiamo c'è ancora da fare un downsample per bilanciare il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive_outcome = df_clean[df_clean[\"Outcome\"] == 1]\n",
    "df_negative_outcome = df_clean[df_clean[\"Outcome\"] == 0]\n",
    "#-----------\n",
    "\n",
    "df_negative_outcome_downsampled = resample(df_negative_outcome,\n",
    "             replace=True,\n",
    "             n_samples=len(df_positive_outcome),\n",
    "             random_state=42)\n",
    "print(df_negative_outcome_downsampled.shape)\n",
    "#-----------------\n",
    "\n",
    "df_balanced = pd.concat([df_negative_outcome_downsampled, df_positive_outcome])\n",
    "#-----------\n",
    "\n",
    "df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Numero di outcome seza diabete\", (df_balanced[\"Outcome\"] == 0).sum())\n",
    "print(\"Numero di outcome con diabete\", (df_balanced[\"Outcome\"] == 1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bene, ora il dataset è bilanciato, non resta che binnarizzarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_balanced:\n",
    "    print(\"valori di \",item)\n",
    "    temp = df_balanced[item].unique()\n",
    "    temp.sort()\n",
    "    #print(df_raw[item].unique())\n",
    "    print(temp)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = df_balanced.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primo approccio alla binnerizzazione, molto manaccione ma funziona. Sotto un apporccio più pulito\n",
    "\n",
    "\"\"\"df_bin[\"Pregnancies\"] = pd.cut(x=df_balanced[\"Pregnancies\"], bins=[-1,3,6,11,18],\n",
    "                        labels=[\"0-3\",\"3-6\",\"6-11\",\"11+\"])\n",
    "\n",
    "df_bin[\"Glucose\"] = pd.cut(x=df_balanced[\"Glucose\"], bins=[70,100,140,np.max(df_balanced[\"Glucose\"])],\n",
    "                        labels=[\"70-100\",\"100-140\",\"140-200\"])\n",
    "\n",
    "df_bin[\"BloodPressure\"] = pd.cut(x=df_balanced[\"BloodPressure\"], bins=[29,60,80,85,90,100,110],\n",
    "                        labels=[\"30-60\",\"60-80\",\"80-85\",\"85-90\",\"90-100\",\"100-110\"])\n",
    "\n",
    "df_bin[\"SkinThickness\"] = pd.cut(x=df_balanced[\"SkinThickness\"], bins=[6,15,30,40,np.max(df_balanced[\"SkinThickness\"])],\n",
    "                        labels=[\"7,15\",\"15,30\",\"30-40\",\"40+\"])\n",
    "\n",
    "df_bin[\"Insulin\"] = pd.cut(x=df_balanced[\"Insulin\"], bins=[-1,25,50,120,150,250,np.max(df_balanced[\"Insulin\"])],\n",
    "                        labels=[\"0-25\",\"25-50\",\"50-120\",\"120-150\",\"150-250\",\"250+\"])\n",
    "\n",
    "df_bin[\"BMI\"] = pd.cut(x=df_balanced[\"BMI\"], bins=[19,25,30,np.max(df_balanced[\"BMI\"])],\n",
    "                        labels=[\"19-25\",\"25-30\",\"30+\"])\n",
    "\n",
    "df_bin[\"DiabetesPedigreeFunction\"] = pd.cut(x=df_balanced[\"DiabetesPedigreeFunction\"], bins=[-1,0.2,0.3,0.5,0.7,0.9,np.max(df_balanced[\"DiabetesPedigreeFunction\"])],\n",
    "                        labels=[\"0-0.2\",\"0.2-0.3\",\"0.3-0.5\",\"0.5-0.7\",\"0.7-0.9\",\"0.9+\"])\n",
    "\n",
    "df_bin[\"Age\"] = pd.cut(x=df_balanced[\"Age\"], bins=[20,30,40,50,60,np.max(df_balanced[\"Age\"])],\n",
    "                        labels=[\"20-30\",\"30-40\",\"40-50\",\"50-60\",\"60+\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary di configurazione per la binnerizzazione\n",
    "\n",
    "conf = {\"Pregnancies\" :{\n",
    "            \"bins\" : [-1,3,6,11,18],\n",
    "            \"labels\": [\"0-3\",\"3-6\",\"6-11\",\"11+\"]},\n",
    "        \"Glucose\" :{\n",
    "            \"bins\" : [70,100,140,np.max(df_balanced[\"Glucose\"])],\n",
    "            \"labels\" : [\"70-100\",\"100-140\",\"140-200\"]},\n",
    "        \"BloodPressure\" :{\n",
    "            \"bins\" : [29,60,80,85,90,100,110],\n",
    "            \"labels\" : [\"30-60\",\"60-80\",\"80-85\",\"85-90\",\"90-100\",\"100-110\"]},\n",
    "        \"SkinThickness\" :{\n",
    "            \"bins\" : [6,15,30,40,np.max(df_balanced[\"SkinThickness\"])],\n",
    "            \"labels\" : [\"7,15\",\"15,30\",\"30-40\",\"40+\"]},\n",
    "        \"Insulin\" : {\n",
    "            \"bins\" : [-1,25,50,120,150,250,np.max(df_balanced[\"Insulin\"])],\n",
    "            \"labels\" : [\"0-25\",\"25-50\",\"50-120\",\"120-150\",\"150-250\",\"250+\"]},\n",
    "        \"BMI\" :{\n",
    "            \"bins\" : [19,25,30,np.max(df_balanced[\"BMI\"])],\n",
    "            \"labels\" : [\"19-25\",\"25-30\",\"30+\"]},\n",
    "        \"DiabetesPedigreeFunction\" :{\n",
    "            \"bins\" : [-1,0.2,0.3,0.5,0.7,0.9,np.max(df_balanced[\"DiabetesPedigreeFunction\"])],\n",
    "            \"labels\" : [\"0-0.2\",\"0.2-0.3\",\"0.3-0.5\",\"0.5-0.7\",\"0.7-0.9\",\"0.9+\"]},\n",
    "        \"Age\" :{\n",
    "            \"bins\" : [20,30,40,50,60,np.max(df_balanced[\"Age\"])],\n",
    "            \"labels\" : [\"20-30\",\"30-40\",\"40-50\",\"50-60\",\"60+\"]}\n",
    "        }       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in conf.keys():\n",
    "    df_bin[item] = pd.cut(x=df_balanced[item], bins=conf[item][\"bins\"],\n",
    "                        labels=conf[item][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_bin:\n",
    "    print(\"valori di \",item)\n",
    "    temp = df_bin[item].unique()\n",
    "    #print(df_raw[item].unique())\n",
    "    print(temp)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I bin sono stati creati correttamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In this chapter both the structure of the network, created from scratch by analyzing scientific papers related to the topic, and the parameter learning process are taken into account.\n",
    "\n",
    "In particular, a variety of methods related to the analysis of Bayesian Networks are explored.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the network\n",
    "\n",
    "A graphical preliminary overview of the network is shown using the visual library of pgmpy, the acronyms related to dataset attributes are the followings:\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import matplotlib.pyplot as plt\n",
    "from daft import PGM\n",
    "\n",
    "pgm = PGM()\n",
    "\n",
    "pgm.add_node(daft.Node('Pr', \"Pr\", 1, 4))\n",
    "pgm.add_node(daft.Node('DPF', \"DPF\", 2, 4))\n",
    "pgm.add_node(daft.Node('Age', \"Age\", 3, 4))\n",
    "\n",
    "pgm.add_node(daft.Node('Diab', \"Diab\", 2, 3))\n",
    "\n",
    "pgm.add_node(daft.Node('Glu', \"Glu\", 1, 2))\n",
    "pgm.add_node(daft.Node('Ins', \"Ins\", 2, 2))\n",
    "pgm.add_node(daft.Node('ST', \"ST\", 3, 2))\n",
    "pgm.add_node(daft.Node('BP', \"BP\", 4, 2))\n",
    "\n",
    "pgm.add_node(daft.Node('BMI', \"BMI\", 1.5, 1))\n",
    "\n",
    "pgm.add_edge('Pr', 'Diab')\n",
    "pgm.add_edge('DPF', 'Diab')\n",
    "pgm.add_edge('Age', 'Diab')\n",
    "pgm.add_edge('Diab', 'Glu')\n",
    "pgm.add_edge('Diab', 'Ins')\n",
    "pgm.add_edge('Diab', 'ST')\n",
    "pgm.add_edge('Diab', 'BP')\n",
    "pgm.add_edge('Glu', 'BMI')\n",
    "pgm.add_edge('Ins', 'BMI')\n",
    "\n",
    "pgm.render()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = PGM()\n",
    "\n",
    "pgm.add_node(daft.Node('Pr', \"Pr\", 1, 3))\n",
    "pgm.add_node(daft.Node('DPF', \"DPF\", 2, 3))\n",
    "pgm.add_node(daft.Node('Age', \"Age\", 3, 3))\n",
    "pgm.add_node(daft.Node('BMI', \"BMI\", 4, 3))\n",
    "\n",
    "pgm.add_node(daft.Node('Diab', \"Diab\", 2, 2))\n",
    "\n",
    "pgm.add_node(daft.Node('Glu', \"Glu\", 1, 1))\n",
    "pgm.add_node(daft.Node('Ins', \"Ins\", 2, 1))\n",
    "pgm.add_node(daft.Node('BP', \"BP\", 3, 1))\n",
    "pgm.add_node(daft.Node('ST', \"ST\", 4, 1))\n",
    "\n",
    "pgm.add_edge('Pr', 'Diab')\n",
    "pgm.add_edge('DPF', 'Diab')\n",
    "pgm.add_edge('Age', 'Diab')\n",
    "pgm.add_edge('BMI','Diab')\n",
    "pgm.add_edge('BMI','ST')\n",
    "pgm.add_edge('BMI', 'BP')\n",
    "pgm.add_edge('Diab', 'Glu')\n",
    "pgm.add_edge('Diab', 'Ins')\n",
    "pgm.add_edge('Diab', 'BP')\n",
    "\n",
    "pgm.render()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of connections between nodes (e.g. direct cause, causal trail) refering to this specific Network are here shown:\n",
    "\n",
    "#Direct cause\n",
    "\n",
    "#Causal trail\n",
    "\n",
    "#Common effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "Connections between nodes have been implemented from scratch refering to a variety of scientific medical sources. Being the causal links in the medical field particularly challenging to model (i.e. often a huge variety of attributes are interlaced, causing directly or indirectly effects on each others) in this work only links that have been considered particularly relevant were defined.\n",
    "\n",
    "Some of the connections are here explained and referenced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Analyzing the network\n",
    "\n",
    "Follows a series of experiments done on the network by applying a variety of methods provided by the pgmpy library in order to see in practice all the concepts addressed during the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all the nodes of the model\n",
    "model.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all the edges of the model\n",
    "model.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local independecies of a single node\n",
    "model.local_independencies(\"high_blood_pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking d-separation between variables with and without evidence\n",
    "# Two sets of nodes X, Y are d-separated given Z if there is no active trail between any X ∈ X and Y ∈ Y given Z\n",
    "\n",
    "print(model.is_dconnected(\"high_blood_pressure\", \"creatinine_phosphokinase\"))\n",
    "print(model.is_dconnected(\"serum_sodium\", \"diabetes\",observed=[\"anaemia\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is_irrelevant made from scratch by the definition of irrelevance given during the course \n",
    "\n",
    "def is_irrelevant (node1,node2,evidence):\n",
    "  if ((node2 in (model.get_ancestral_graph(node1)and model.get_ancestral_graph(evidence))) and model.is_dconnected(node1, node2,observed=evidence )):\n",
    "    print(\"The node {0} is not irrelevant with resepect to {1}, given the evidence {2}.\".format(node1, node2, evidence))\n",
    "  else:\n",
    "    print(\"The node {0} is irrelevant with resepect to {1}, given the evidence {2}\".format(node1, node2, evidence))\n",
    "\n",
    "\n",
    "is_irrelevant(\"ejection_fraction\", \"diabetes\", \"heart_failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking active trails from high blood pressure given the evidence diabetes\n",
    "model.active_trail_nodes('high_blood_pressure', observed='diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the markov blanket of the node heart failure\n",
    "model.get_markov_blanket(\"heart_failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter learning\n",
    "\n",
    "Learning Bayesian networks from data, knowing the structure of the network, boils down to parameter estimation. In pgmpy a variety of estimators is available, going from IVestimator to SEMestimator, but the main choice for this problem fell on two of them: Maximum Likelihood and Bayesian Estimator.\n",
    "\n",
    "MLE, which depends solely on the outcomes of observed data, could be a reasonable simple starting point, however, it is notorious for becoming easily biased when the data is minimal. Moreover, in situations where observed data is sparse, Bayesian estimation’s incorporation of prior knowledge can help in attaining a more accurate model. \n",
    "\n",
    "On the other hand, unreliable priors can lead to a slippery slope of highly biased models that require large amounts of seen data to remedy; so priors need to be well defined and contain relevant insight to the problem in order to avoid that. \n",
    "\n",
    "Taken into consideration pros and cons of both approaches, given the minimal dataset used for this work the Bayesian Estimator is choosen.\n",
    "\n",
    "Pgmpy also offer a variety of possible priors, among them a possible choice is Bayesian Dirichlet equivalent uniform prior (BDeu), choosen with its default settings (i.e. equivalent_sample_size equal to 5).\n",
    "\n",
    "In the following cell the parameter estimation process takes place and the learned CPTs are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "model.cpds = []\n",
    "model.fit(data=df, estimator=BayesianEstimator, prior_type=\"BDeu\")\n",
    "\n",
    "for cpd in model.get_cpds():\n",
    "    print(f'CPT of {cpd.variable}:')\n",
    "    print(cpd, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the validity of the model \n",
    "# This method checks if the sum of the probabilities for each state is equal to 1 (tol=0.01) and if the CPDs associated with nodes are consistent with their parents.\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ML4downsyndrome-D1DocbTt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e765655f724514810ca5f5619b481f72aea2afcac92a9986adb907bce56ced13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
