{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIKR module 3 project\n",
    "\n",
    "#Lorenzo Venieri\n",
    "#Luca Zucchini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Diabetes is a serious disease affecting millions of people across the entire world. Thus, correct and timely prediction of this disease is very important due to the complications it can have in the case of other life-threatening diseases.\n",
    "\n",
    "Objective of this project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "-Aggiungere funzione per trovare correlazione tra features\n",
    "-Considerare il fatto che il diabete causa durezza della pelle, ma quest'ultima non Ã¨ direttamente causa di diabete (=> != <=>)+\n",
    "-Piccola spiegazione del ragionamento dietro le correlazioni NON trovate\n",
    "-Fattore di rischio\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pgmpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "768 rows, 9 columns\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney\n",
    "Diseases. The objective of the dataset is to diagnostically predict whether a patient has diabetes,\n",
    "based on certain diagnostic measurements included in the dataset. Several constraints were placed\n",
    "on the selection of these instances from a larger database. In particular, all patients here are females\n",
    "at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "From the data set in the (.csv) file we can find several variables, some of them are independent\n",
    "(several medical predictor variables) and only one target dependent variable (Outcome).\n",
    "\n",
    "We have 9 different attributes:\n",
    "1. Pregnancies: number of pregnancies\n",
    "2. Glucose: plasma glucose concentrarion \n",
    "3. BloodPressure: diastolic blood pressure mm/Hg\n",
    "4. SkinThickness: triceps skin fold thickness (mm)\n",
    "5. Insulin: insulin in U/mL\n",
    "6. BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes Pedigree Function: scores likelihood of diabetes based on family history\n",
    "8. Age: age of the person (years)\n",
    "9. Outcome: patient has diabetes (0 = No, 1 = Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_raw = pd.read_csv(\"data/diabetes.csv\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diabetes distribution in the dataset\n",
    "\n",
    "df_raw['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset there are 268 patients with diabetes and 500 without. The classes are imbalanced: only about 35% of the entries have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of the other 8 (continuous) features\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(18,16))\n",
    "axs = axs.flatten()\n",
    "sns.distplot(df_raw['Pregnancies'],rug=True,ax=axs[0])\n",
    "sns.distplot(df_raw['Glucose'],rug=True,ax=axs[1])\n",
    "sns.distplot(df_raw['BloodPressure'],rug=True,ax=axs[2])\n",
    "sns.distplot(df_raw['SkinThickness'],rug=True,ax=axs[3])\n",
    "sns.distplot(df_raw['Insulin'],rug=True,ax=axs[4])\n",
    "sns.distplot(df_raw['BMI'],rug=True,ax=axs[5])\n",
    "sns.distplot(df_raw['DiabetesPedigreeFunction'],rug=True,ax=axs[6])\n",
    "sns.distplot(df_raw['Age'],rug=True,ax=axs[7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che ci sono tanti valori nulli nel dataset. Sarebbero da togliere dato che sono dei NaN. Allo stesso tempo possiamo usare questa cosa per iniziare un downsample dei dati con Outcome '0'\n",
    "\n",
    "In the dataset there are many null values that we have to handle. We decided to remove them instead of using statistical methods to predict the missing values, since we have few data and we want them to be as much predictive as possible. By the way, we have to consider that null values of Insulin are interesting data since they usually indicate the presence of Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_raw.columns :\n",
    "    if item != \"Outcome\":\n",
    "        print(\"In the column\", item,\"there are\",(df_raw[item].values == 0).sum(),\"null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOGLIERE\n",
    "for item in df_raw.columns:\n",
    "    print(\"valori di \",item)\n",
    "    temp = df_raw[item].unique()\n",
    "    temp.sort()\n",
    "    #print(df_raw[item].unique())\n",
    "    print(temp)\n",
    "    print(\"----------------------\")\n",
    "    #\n",
    "    #chiedere a Sten dei bin sensati per questi valori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with the removal of rows that have a null value in the columns: 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'.\n",
    "Previous analysis showed that DiabetesPedigreeFunction column and Age column don't contain any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness','BMI']\n",
    "for item in columns_to_clean:\n",
    "    drop_list = []\n",
    "    for i in range(len(df_clean[item])):\n",
    "        if df_clean[item][i] == 0 :\n",
    "            drop_list.append(i)\n",
    "    print(\"Column\",item,\"has been cleaned\")\n",
    "    df_clean.drop(labels= drop_list, axis= 0, inplace= True)\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regard to the Insulin column we consider the fact that a null value of insulin in the blood is a sure sign of the presence of (type 1) Diabetes, so we proceed with removing all the rows that have a null value of both Insuline and Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_clean[\"Insulin\"])):\n",
    "    if df_clean[\"Insulin\"][i] == 0 and df_clean[\"Outcome\"][i] == 0:\n",
    "        df_clean.drop(labels= i, axis= 0, inplace= True)\n",
    "print(\"Column 'Insulin' has been cleaned\")\n",
    "df_clean.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check now if the dataset is still unbalanced with respect to the Outcome column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of Outcomes without diabetes\", (df_clean[\"Outcome\"] == 0).sum())\n",
    "print(\"Total number of Outcomes with diabetes\", (df_clean[\"Outcome\"] == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllo sull'insulina e sul tipo di diabete\n",
    "\n",
    "#TOGLIERLO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = 0\n",
    "count_2 = 1\n",
    "for i in range(df_clean.shape[0]):\n",
    "    if df_clean[\"Insulin\"][i] < 1 and df_clean[\"Outcome\"][i] == 1 :\n",
    "        count_1 += 1\n",
    "    if df_clean[\"Insulin\"][i] > 200 and df_clean[\"Outcome\"][i] == 1:\n",
    "        count_2 += 1\n",
    "print(\"presumibilmente ci sono \",count_1,\"pazienti con diabete di tipo 1\")\n",
    "print(\"presumibilmente ci sono \",count_2,\"pazienti con diabete di tipo 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice there is the need of a balancing of the dataset since the '0' outcomes are 50% more than the '1' outcomes. We deal with this using the resample method provided by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive_outcome = df_clean[df_clean[\"Outcome\"] == 1]\n",
    "df_negative_outcome = df_clean[df_clean[\"Outcome\"] == 0]\n",
    "#-----------\n",
    "\n",
    "df_negative_outcome_downsampled = resample(df_negative_outcome,\n",
    "             replace=True,\n",
    "             n_samples=len(df_positive_outcome),\n",
    "             random_state=42)\n",
    "print(df_negative_outcome_downsampled.shape)\n",
    "#-----------------\n",
    "\n",
    "df_balanced = pd.concat([df_negative_outcome_downsampled, df_positive_outcome])\n",
    "#-----------\n",
    "\n",
    "df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Total number of Outcomes without diabetes\", (df_balanced[\"Outcome\"] == 0).sum())\n",
    "print(\"Total number of Outcomes with diabetes\", (df_balanced[\"Outcome\"] == 1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a balanced dataset, the next step is to modify the values of the columns from continuous values to discrete values. This transition is necessary in order to .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_balanced:\n",
    "    print(\"valori di \",item)\n",
    "    temp = df_balanced[item].unique()\n",
    "    temp.sort()\n",
    "    #print(df_raw[item].unique())\n",
    "    print(temp)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binnerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = df_balanced.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primo approccio alla binnerizzazione, molto manaccione ma funziona. Sotto un apporccio piÃ¹ pulito\n",
    "\n",
    "\"\"\"df_bin[\"Pregnancies\"] = pd.cut(x=df_balanced[\"Pregnancies\"], bins=[-1,3,6,11,18],\n",
    "                        labels=[\"0-3\",\"3-6\",\"6-11\",\"11+\"])\n",
    "\n",
    "df_bin[\"Glucose\"] = pd.cut(x=df_balanced[\"Glucose\"], bins=[70,100,140,np.max(df_balanced[\"Glucose\"])],\n",
    "                        labels=[\"70-100\",\"100-140\",\"140-200\"])\n",
    "\n",
    "df_bin[\"BloodPressure\"] = pd.cut(x=df_balanced[\"BloodPressure\"], bins=[29,60,80,85,90,100,110],\n",
    "                        labels=[\"30-60\",\"60-80\",\"80-85\",\"85-90\",\"90-100\",\"100-110\"])\n",
    "\n",
    "df_bin[\"SkinThickness\"] = pd.cut(x=df_balanced[\"SkinThickness\"], bins=[6,15,30,40,np.max(df_balanced[\"SkinThickness\"])],\n",
    "                        labels=[\"7,15\",\"15,30\",\"30-40\",\"40+\"])\n",
    "\n",
    "df_bin[\"Insulin\"] = pd.cut(x=df_balanced[\"Insulin\"], bins=[-1,25,50,120,150,250,np.max(df_balanced[\"Insulin\"])],\n",
    "                        labels=[\"0-25\",\"25-50\",\"50-120\",\"120-150\",\"150-250\",\"250+\"])\n",
    "\n",
    "df_bin[\"BMI\"] = pd.cut(x=df_balanced[\"BMI\"], bins=[19,25,30,np.max(df_balanced[\"BMI\"])],\n",
    "                        labels=[\"19-25\",\"25-30\",\"30+\"])\n",
    "\n",
    "df_bin[\"DiabetesPedigreeFunction\"] = pd.cut(x=df_balanced[\"DiabetesPedigreeFunction\"], bins=[-1,0.2,0.3,0.5,0.7,0.9,np.max(df_balanced[\"DiabetesPedigreeFunction\"])],\n",
    "                        labels=[\"0-0.2\",\"0.2-0.3\",\"0.3-0.5\",\"0.5-0.7\",\"0.7-0.9\",\"0.9+\"])\n",
    "\n",
    "df_bin[\"Age\"] = pd.cut(x=df_balanced[\"Age\"], bins=[20,30,40,50,60,np.max(df_balanced[\"Age\"])],\n",
    "                        labels=[\"20-30\",\"30-40\",\"40-50\",\"50-60\",\"60+\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary di configurazione per la binnerizzazione\n",
    "\n",
    "conf = {\"Pregnancies\" :{\n",
    "            \"bins\" : [-1,3,6,11,18],\n",
    "            \"labels\": [\"0-3\",\"3-6\",\"6-11\",\"11+\"]},\n",
    "\n",
    "        \"Glucose\" :{\n",
    "            \"bins\" : [70,100,140,np.max(df_balanced[\"Glucose\"])],\n",
    "            \"labels\" : [\"70-100\",\"100-140\",\"140-200\"]},\n",
    "\n",
    "        \"BloodPressure\" :{\n",
    "            \"bins\" : [29,60,80,85,90,100,110],\n",
    "            \"labels\" : [\"30-60\",\"60-80\",\"80-85\",\"85-90\",\"90-100\",\"100-110\"]},\n",
    "\n",
    "        \"SkinThickness\" :{\n",
    "            \"bins\" : [6,15,30,40,np.max(df_balanced[\"SkinThickness\"])],\n",
    "            \"labels\" : [\"7-15\",\"15-30\",\"30-40\",\"40+\"]},\n",
    "\n",
    "        \"Insulin\" : {\n",
    "            \"bins\" : [-1,25,50,120,150,250,np.max(df_balanced[\"Insulin\"])],\n",
    "            \"labels\" : [\"0-25\",\"25-50\",\"50-120\",\"120-150\",\"150-250\",\"250+\"]},\n",
    "            \n",
    "        \"BMI\" :{\n",
    "            \"bins\" : [19,25,30,np.max(df_balanced[\"BMI\"])],\n",
    "            \"labels\" : [\"19-25\",\"25-30\",\"30+\"]},\n",
    "\n",
    "        \"DiabetesPedigreeFunction\" :{\n",
    "            \"bins\" : [-1,0.2,0.3,0.5,0.7,0.9,np.max(df_balanced[\"DiabetesPedigreeFunction\"])],\n",
    "            \"labels\" : [\"0-0.2\",\"0.2-0.3\",\"0.3-0.5\",\"0.5-0.7\",\"0.7-0.9\",\"0.9+\"]},\n",
    "            \n",
    "        \"Age\" :{\n",
    "            \"bins\" : [20,30,40,50,60,np.max(df_balanced[\"Age\"])],\n",
    "            \"labels\" : [\"20-30\",\"30-40\",\"40-50\",\"50-60\",\"60+\"]}\n",
    "        }       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in conf.keys():\n",
    "    df_bin[item] = pd.cut(x=df_balanced[item], bins=conf[item][\"bins\"],\n",
    "                        labels=conf[item][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_bin:\n",
    "    print(\"valori di \",item)\n",
    "    temp = df_bin[item].unique()\n",
    "    #print(df_raw[item].unique())\n",
    "    print(temp)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of the binerized features distributions\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(18,20))\n",
    "axs = axs.flatten()\n",
    "for n,column_name in enumerate(df_bin.columns):\n",
    "    df_bin[column_name].value_counts().plot(kind='bar', title = column_name, ax = axs[n-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I bin sono stati creati correttamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will build the network by analyzing correlations between the columns and scientific papers related to the topic. We will then explore a variety of methods related to the analysis of Bayesian Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the correlations between the columns of the dataframe can help us reason about the causal relationships between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_clean.corr()\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age-pregnancies\n",
    "Outcome-glucose\n",
    "insulin-glucose\n",
    "insulin-skinthickness\n",
    "bmi-skinthickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the network\n",
    "\n",
    "We show a graphical preliminary overview of the network using the visual library daft of pgmpy.  \n",
    "The acronyms related to dataset attributes are the followings:\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import matplotlib.pyplot as plt\n",
    "from daft import PGM\n",
    "\n",
    "pgm = PGM()\n",
    "\n",
    "pgm.add_node(daft.Node('Age', \"Age\", 2, 4))\n",
    "pgm.add_node(daft.Node('DPF', \"DPF\", 3, 4))\n",
    "pgm.add_node(daft.Node('BMI', \"BMI\", 4, 4))\n",
    "\n",
    "pgm.add_node(daft.Node('Pr', \"Pr\", 1, 3.5))\n",
    "\n",
    "pgm.add_node(daft.Node('Diab', \"Diab\", 2, 3))\n",
    "\n",
    "pgm.add_node(daft.Node('Glu', \"Glu\", 1, 2))\n",
    "pgm.add_node(daft.Node('Ins', \"Ins\", 2, 2))\n",
    "\n",
    "pgm.add_node(daft.Node('BP', \"BP\", 3, 1))\n",
    "pgm.add_node(daft.Node('ST', \"ST\", 4, 1))\n",
    "\n",
    "pgm.add_edge('Age', 'Pr')\n",
    "pgm.add_edge('Pr', 'Diab')\n",
    "pgm.add_edge('DPF', 'Diab')\n",
    "pgm.add_edge('Age', 'Diab')\n",
    "pgm.add_edge('BMI','Diab')\n",
    "pgm.add_edge('BMI','ST')\n",
    "pgm.add_edge('BMI', 'BP')\n",
    "pgm.add_edge('Diab', 'Glu')\n",
    "pgm.add_edge('Diab', 'Ins')\n",
    "pgm.add_edge('Glu', 'BP')\n",
    "pgm.add_edge('Ins', 'BP')\n",
    "\n",
    "pgm.render()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of connections between nodes (e.g. direct cause, causal trail) refering to this specific Network are here shown:\n",
    "\n",
    "#Direct cause\n",
    "\n",
    "#Causal trail\n",
    "\n",
    "#Common effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "Connections between nodes have been implemented from scratch refering to a variety of scientific medical sources. Being the causal links in the medical field particularly challenging to model (i.e. often a huge variety of attributes are interlaced, causing directly or indirectly effects on each others) in this work only links that have been considered particularly relevant were defined.\n",
    "\n",
    "Some of the connections are here explained and referenced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgmpy\n",
    "from pgmpy.models import BayesianNetwork\n",
    "\n",
    "model = BayesianNetwork([('Age','Pregnancies'),('Pregnancies', 'Diabetes'),('DiabetesPedigreeFunction', 'Diabetes'),('Age', 'Diabetes'),('BMI','Diabetes'),\n",
    "('BMI','SkinThickness'),('BMI', 'BloodPressure'),('Diabetes', 'Glucose'),('Diabetes', 'Insulin'),('Glucose', 'BloodPressure'),('Insulin','BloodPressure')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Analyzing the network\n",
    "\n",
    "Follows a series of experiments done on the network by applying a variety of methods provided by the pgmpy library in order to see in practice all the concepts addressed during the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all the nodes of the model\n",
    "model.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all the edges of the model\n",
    "model.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local independencies of a single node\n",
    "model.local_independencies(\"Diabetes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.local_independencies(\"SkinThickness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking d-separation between variables with and without evidence\n",
    "# Two sets of nodes X, Y are d-separated given Z if there is no active trail between any x â X and y â Y given Z\n",
    "\n",
    "print(model.is_dconnected(\"Glucose\", \"Insuline\"))\n",
    "print(model.is_dconnected(\"Glucose\", \"Insuline\",observed=[\"Diabetes\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is_irrelevant recalls the definition of irrelevance given during the course \n",
    "\n",
    "def is_irrelevant(node1,node2,evidence):\n",
    "  if ((node2 in (model.get_ancestral_graph(node1)and model.get_ancestral_graph(evidence))) and model.is_dconnected(node1, node2,observed=evidence )):\n",
    "    print(\"Node\", node1, \"is not irrelevant with resepect to\", node2, \"given the evidence\", evidence)\n",
    "  else:\n",
    "    print(\"Node\", node1, \"is irrelevant with resepect to\", node2, \"given the evidence\", evidence)\n",
    "\n",
    "is_irrelevant(\"Glucose\", \"Insuline\", \"Diabetes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking active trails from blood pressure given the evidence diabetes\n",
    "model.active_trail_nodes('BloodPressure', observed='Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the markov blanket of the node BloodPressure\n",
    "model.get_markov_blanket(\"BloodPressure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter learning\n",
    "\n",
    "Learning Bayesian networks from data, knowing the structure of the network, boils down to parameter estimation. In pgmpy a variety of estimators is available, going from IVestimator to SEMestimator, but the main choice for this problem fell on two of them: Maximum Likelihood and Bayesian Estimator.\n",
    "\n",
    "MLE, which depends solely on the outcomes of observed data, could be a reasonable simple starting point, however, it is notorious for becoming easily biased when the data is minimal. Moreover, in situations where observed data is sparse, Bayesian estimationâs incorporation of prior knowledge can help in attaining a more accurate model. \n",
    "\n",
    "On the other hand, unreliable priors can lead to a slippery slope of highly biased models that require large amounts of seen data to remedy; so priors need to be well defined and contain relevant insight to the problem in order to avoid that. \n",
    "\n",
    "Taken into consideration pros and cons of both approaches, given the minimal dataset used for this work the Bayesian Estimator is choosen.\n",
    "\n",
    "Pgmpy also offer a variety of possible priors, among them a possible choice is Bayesian Dirichlet equivalent uniform prior (BDeu), choosen with its default settings (i.e. equivalent_sample_size equal to 5).\n",
    "\n",
    "In the following cell the parameter estimation process takes place and the learned CPTs are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_bin.rename(columns = {'Outcome':'Diabetes'}) #column names and node names must be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpds = []\n",
    "model.fit(data=df, estimator=BayesianEstimator, prior_type=\"BDeu\")\n",
    "\n",
    "for cpd in model.get_cpds():\n",
    "    print(f'CPT of {cpd.variable}:')\n",
    "    print(cpd, '\\n')\n",
    "model.cpds = []\n",
    "model.fit(data=df, estimator=BayesianEstimator, prior_type=\"BDeu\")\n",
    "\n",
    "for cpd in model.get_cpds():\n",
    "    print(f'CPT of {cpd.variable}:')\n",
    "    print(cpd, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the validity of the model \n",
    "# This method checks if the sum of the probabilities for each state is equal to 1 (tol=0.01) and if the CPDs associated with nodes are consistent with their parents.\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cardinality of all model nodes\n",
    "model.get_cardinality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "Pgmpy allows to develop both exact and approximate inference on the Bayesian Network, in particular in this section both are presented exploring a variety of different methods related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact inference\n",
    "\n",
    "Exact Inference in pgmpy is implemented through the Variable Elimination Method and Belief Propagation rather than with simple enumeration; being one of the purpouses of this work exploring topics seen in class, the former is choosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "infer = VariableElimination(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Causal inference (prediction): probability that the patient has diabetes given her number of pregnancies\n",
    "print('Probability of having diabetes with 0 to 3 pregnancies:')\n",
    "print(infer.query([\"Diabetes\"],evidence={\"Pregnancies\": '0-3'}))\n",
    "\n",
    "print('Probability of having diabetes with more than 11 pregnancies:')\n",
    "print(infer.query([\"Diabetes\"],evidence={\"Pregnancies\": '11+'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evidential inference (explanation): probability of having diabetes given that the patient has very high insulin level\n",
    "#TODO add evidence of insulin = 0\n",
    "print(infer.query([\"Diabetes\"],evidence={\"Insulin\": '250+'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intercausal inference (explaining away): why she has high blood pressure given the fact that she hasn't diabetes\n",
    "print(infer.query([\"BMI\"],evidence={\"BloodPressure\": '100-110' , \"Diabetes\": 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum a posteriori (MAP) queries find the most probable configuration (called MAP configuration) of some specific variables of interest (also called MAP variable), given observations of some evidence variables.\n",
    "The result of MAP queries is (in case of discrete random variables) a single value rather than a probability which encodes the instantiation of the MAP variable with higher probability given the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infer MAP queries\n",
    "print(infer.map_query([\"Insulin\"]))\n",
    "print(infer.map_query([\"Insulin\"], evidence={'Glucose': '70-100'}))\n",
    "print(infer.map_query([\"Insulin\"], evidence={'Diabetes': 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import ApproxInference\n",
    "from pgmpy.sampling import BayesianModelSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = ApproxInference(model)\n",
    "inference = BayesianModelSampling(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from an empty network\n",
    "print(infer.query(variables=[\"Diabetes\"], n_samples=100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the convergence increasing number of samples\n",
    "print(infer.query(variables=[\"BloodPressure\"], n_samples=10)) #approximate probability with a low number of samples\n",
    "\n",
    "print(infer.query(variables=[\"BloodPressure\"], n_samples=10000)) #approximate probability with an high number of samples\n",
    "\n",
    "print(VariableElimination(model).query([\"BloodPressure\"]))      #true probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer.query(variables=[\"Diabetes\"], n_samples=10)) #approximate probability with a low number of samples\n",
    "\n",
    "print(infer.query(variables=[\"Diabetes\"], n_samples=100)) #approximate probability with a medium number of samples\n",
    "\n",
    "print(infer.query(variables=[\"Diabetes\"], n_samples=10000)) #approximate probability with an high number of samples\n",
    "\n",
    "print('True probability:')\n",
    "print(VariableElimination(model).query([\"Diabetes\"]))      #true probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that with a small number of samples we don't even find all the possible events for BloodPressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood weighting\n",
    "#let's see the results of a sampling process\n",
    "print(inference.likelihood_weighted_sample( size=10)) #size: number of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observe the distribution obtained to derive the approx probabilities\n",
    "print(infer.get_distribution(inference.likelihood_weighted_sample( size=1000), [\"Diabetes\"], joint=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejection sampling\n",
    "print(infer.get_distribution(inference.rejection_sample( size=1000), [\"Diabetes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that, given the same number of samples, sampling from an empty network, likelihood weighting and rejection sampling give us different results. This was expected as we would need more samples to reach convergence for all theese three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Now that we have explored some functionalities of pgmpy to deal with our network we will explore our model to see if we can get some useful insights about diabetes and the related fetures from what the model has learned using the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ML4downsyndrome-D1DocbTt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e765655f724514810ca5f5619b481f72aea2afcac92a9986adb907bce56ced13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
